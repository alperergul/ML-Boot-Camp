{"cells":[{"cell_type":"markdown","metadata":{"id":"2tudJDX5MQ3D"},"source":["# What we're covering in the Scikit-Learn Introduction\n","\n","This notebook outlines the content convered in the Scikit-Learn Introduction.\n","\n","It's a quick stop to see all the Scikit-Learn functions and modules for each section outlined.\n","\n","What we're covering follows the following diagram detailing a Scikit-Learn workflow.\n","\n","<img src=\"https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/images/sklearn-workflow-title.png?raw=1\"/>"]},{"cell_type":"markdown","metadata":{"id":"qpvkBzkQMQ3G"},"source":["## 0. Standard library imports\n","\n","For all machine learning projects, you'll often see these libraries (Matplotlib, NumPy and pandas) imported at the top."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"uhk6lNPCMQ3G"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"3jETJGE5MQ3H"},"source":["We'll use 2 datasets for demonstration purposes.\n","* `heart_disease` - a classification dataset (predicting whether someone has heart disease or not)\n","* `boston_df` - a regression dataset (predicting the median house prices of cities in Boston)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"-E5t5NiVMQ3H"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\alper\\AlperProjects\\UDEMY\\ML\\04-scikit-learn\\env\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n","\n","    The Boston housing prices dataset has an ethical problem. You can refer to\n","    the documentation of this function for further details.\n","\n","    The scikit-learn maintainers therefore strongly discourage the use of this\n","    dataset unless the purpose of the code is to study and educate about\n","    ethical issues in data science and machine learning.\n","\n","    In this special case, you can fetch the dataset from the original\n","    source::\n","\n","        import pandas as pd\n","        import numpy as np\n","\n","        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n","        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n","        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n","        target = raw_df.values[1::2, 2]\n","\n","    Alternative datasets include the California housing dataset (i.e.\n","    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n","    dataset. You can load the datasets as follows::\n","\n","        from sklearn.datasets import fetch_california_housing\n","        housing = fetch_california_housing()\n","\n","    for the California housing dataset and::\n","\n","        from sklearn.datasets import fetch_openml\n","        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n","\n","    for the Ames housing dataset.\n","  warnings.warn(msg, category=FutureWarning)\n"]}],"source":["# Classification data\n","heart_disease = pd.read_csv(\"heart-disease.csv\")\n","\n","# Regression data\n","from sklearn.datasets import load_boston\n","boston = load_boston() # loads as dictionary\n","# Convert dictionary to dataframe\n","boston_df = pd.DataFrame(boston[\"data\"], columns=boston[\"feature_names\"])\n","boston_df[\"target\"] = pd.Series(boston[\"target\"])"]},{"cell_type":"markdown","metadata":{"id":"ZNN5MtzpMQ3I"},"source":["## 1. Get the data ready"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KWlXBotvMQ3I"},"outputs":[],"source":["# Split data into X & y\n","X = heart_disease.drop(\"target\", axis=1) # use all columns except target\n","y = heart_disease[\"target\"] # we want to predict y using X"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"n8aeFKSmMQ3J"},"outputs":[],"source":["# Split the data into training and test sets\n","from sklearn.model_selection import train_test_split\n","# Example use case (requires X & y)\n","X_train, X_test, y_train, y_test = train_test_split(X, y)"]},{"cell_type":"markdown","metadata":{"id":"Th5avu8yMQ3J"},"source":["## 2. Pick a model/estimator (to suit your problem)\n","To pick a model we use the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n","\n","<img src=\"https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/images/sklearn-ml-map.png?raw=1\" width=400/>\n","\n","**Note:** Scikit-Learn refers to machine learning models and algorithms as estimators."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"OZlo-vR8MQ3K"},"outputs":[],"source":["# Random Forest Classifier (for classification problems)\n","from sklearn.ensemble import RandomForestClassifier\n","# Instantiating a Random Forest Classifier (clf short for classifier)\n","clf = RandomForestClassifier()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"foaTrBSLMQ3K"},"outputs":[],"source":["# Random Forest Regressor (for regression problems)\n","from sklearn.ensemble import RandomForestRegressor\n","# Instantiating a Random Forest Regressor\n","model = RandomForestRegressor()"]},{"cell_type":"markdown","metadata":{"id":"lopr-cEPMQ3K"},"source":["## 3. Fit the model to the data and make a prediction\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"gCSg2yIBMQ3L","outputId":"f6619f10-7b57-4df1-888e-4ccef7c108c3"},"outputs":[{"data":{"text/plain":["(array([1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n","        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n","        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n","        0, 1, 1, 0, 0, 0, 1, 1, 0, 1], dtype=int64),\n"," array([[0.33, 0.67],\n","        [0.89, 0.11],\n","        [0.87, 0.13],\n","        [0.06, 0.94],\n","        [0.06, 0.94],\n","        [0.48, 0.52],\n","        [0.21, 0.79],\n","        [0.15, 0.85],\n","        [0.75, 0.25],\n","        [0.57, 0.43],\n","        [0.4 , 0.6 ],\n","        [0.24, 0.76],\n","        [0.97, 0.03],\n","        [0.76, 0.24],\n","        [0.26, 0.74],\n","        [0.74, 0.26],\n","        [0.25, 0.75],\n","        [0.03, 0.97],\n","        [0.16, 0.84],\n","        [0.65, 0.35],\n","        [0.7 , 0.3 ],\n","        [0.38, 0.62],\n","        [0.33, 0.67],\n","        [0.89, 0.11],\n","        [0.33, 0.67],\n","        [0.58, 0.42],\n","        [0.49, 0.51],\n","        [0.92, 0.08],\n","        [0.66, 0.34],\n","        [0.96, 0.04],\n","        [0.07, 0.93],\n","        [0.23, 0.77],\n","        [0.49, 0.51],\n","        [0.79, 0.21],\n","        [0.05, 0.95],\n","        [0.17, 0.83],\n","        [0.41, 0.59],\n","        [0.99, 0.01],\n","        [0.73, 0.27],\n","        [0.19, 0.81],\n","        [0.14, 0.86],\n","        [0.74, 0.26],\n","        [1.  , 0.  ],\n","        [0.23, 0.77],\n","        [0.08, 0.92],\n","        [0.56, 0.44],\n","        [0.13, 0.87],\n","        [0.09, 0.91],\n","        [0.12, 0.88],\n","        [0.03, 0.97],\n","        [0.52, 0.48],\n","        [0.84, 0.16],\n","        [0.77, 0.23],\n","        [0.94, 0.06],\n","        [0.41, 0.59],\n","        [0.52, 0.48],\n","        [0.08, 0.92],\n","        [0.57, 0.43],\n","        [0.37, 0.63],\n","        [0.85, 0.15],\n","        [0.53, 0.47],\n","        [0.12, 0.88],\n","        [0.9 , 0.1 ],\n","        [0.16, 0.84],\n","        [0.29, 0.71],\n","        [0.83, 0.17],\n","        [0.99, 0.01],\n","        [0.09, 0.91],\n","        [0.2 , 0.8 ],\n","        [0.94, 0.06],\n","        [0.93, 0.07],\n","        [0.84, 0.16],\n","        [0.24, 0.76],\n","        [0.43, 0.57],\n","        [0.93, 0.07],\n","        [0.19, 0.81]]))"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# All models/estimators have the fit() function built-in\n","clf.fit(X_train, y_train)\n","\n","# Once fit is called, you can make predictions using predict()\n","y_preds = clf.predict(X_test)\n","\n","# You can also predict with probabilities (on classification models)\n","y_probs = clf.predict_proba(X_test)\n","\n","# View preds/probabilities\n","y_preds, y_probs"]},{"cell_type":"markdown","metadata":{"id":"mqJoWsjcMQ3L"},"source":["## 4. Evaluate the model\n","\n","Every Scikit-Learn model has a default metric which is accessible through the `score()` function.\n","\n","However there are a range of different evaluation metrics you can use depending on the model you're using.\n","\n","A full list of evaluation metrics can be [found in the documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"NnYmwljzMQ3M","outputId":"62637e22-df46-4c78-ff41-d47884a038a1"},"outputs":[{"data":{"text/plain":["0.75"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# All models/estimators have a score() function\n","clf.score(X_test, y_test)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"-XoZxBGbMQ3M","outputId":"d71bf519-9ace-495e-b5e9-ad9017b6c221"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.83606557 0.86885246 0.81967213 0.83333333 0.76666667]\n","[0.81081081 0.90625    0.8125     0.84375    0.76315789]\n"]}],"source":["# Evaluting a model using cross-validation is possible with cross_val_score\n","from sklearn.model_selection import cross_val_score\n","\n","# scoring=None means default score() metric is used\n","print(cross_val_score(estimator=clf, \n","                      X=X, \n","                      y=y, \n","                      cv=5, # use 5-fold cross-validation\n","                      scoring=None)) \n","\n","# Evaluate a model with a different scoring method\n","print(cross_val_score(estimator=clf, \n","                      X=X, \n","                      y=y,\n","                      cv=5, # use 5-fold cross-validation\n","                      scoring=\"precision\"))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"HQAJOuyAMQ3M","outputId":"6d487eb6-50b7-4517-b8a1-207d16a44613"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.75\n","0.7570028011204482\n","[[29 13]\n"," [ 6 28]]\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.69      0.75        42\n","           1       0.68      0.82      0.75        34\n","\n","    accuracy                           0.75        76\n","   macro avg       0.76      0.76      0.75        76\n","weighted avg       0.76      0.75      0.75        76\n","\n"]}],"source":["# Different classification metrics\n","\n","# Accuracy\n","from sklearn.metrics import accuracy_score\n","print(accuracy_score(y_test, y_preds))\n","\n","# Reciver Operating Characteristic (ROC curve)/Area under curve (AUC)\n","from sklearn.metrics import roc_curve, roc_auc_score\n","false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probs[:, 1])\n","print(roc_auc_score(y_test, y_preds))\n","\n","# Confusion matrix\n","from sklearn.metrics import confusion_matrix\n","print(confusion_matrix(y_test, y_preds))\n","\n","# Classification report\n","from sklearn.metrics import classification_report\n","print(classification_report(y_test, y_preds))"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"3N-rB4TaMQ3M","outputId":"9da63761-d4ad-48fe-9fcf-f2c925625121"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8778802888762944\n","1.9697352941176467\n","6.875912539215684\n"]}],"source":["# Different regression metrics\n","\n","# Make predictions first\n","X = boston_df.drop(\"target\", axis=1)\n","y = boston_df[\"target\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","model = RandomForestRegressor()\n","model.fit(X_train, y_train)\n","y_preds = model.predict(X_test)\n","\n","# R^2 (pronounced r-squared) or coefficient of determination\n","from sklearn.metrics import r2_score\n","print(r2_score(y_test, y_preds))\n","\n","# Mean absolute error (MAE)\n","from sklearn.metrics import mean_absolute_error\n","print(mean_absolute_error(y_test, y_preds))\n","\n","# Mean square error (MSE)\n","from sklearn.metrics import mean_squared_error\n","print(mean_squared_error(y_test, y_preds))"]},{"cell_type":"markdown","metadata":{"id":"oQq1vmrfMQ3N"},"source":["## 5. Improve through experimentation\n","\n","Two of the main methods to improve a models baseline metrics (the first evaluation metrics you get).\n","\n","From a data perspective asks:\n","* Could we collect more data? In machine learning, more data is generally better, as it gives a model more opportunities to learn patterns.\n","* Could we improve our data? This could mean filling in misisng values or finding a better encoding (turning things into numbers) strategy.\n","\n","From a model perspective asks:\n","* Is there a better model we could use? If you've started out with a simple model, could you use a more complex one? (we saw an example of this when looking at the [Scikit-Learn machine learning map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html), ensemble methods are generally considered more complex models)\n","* Could we improve the current model? If the model you're using performs well straight out of the box, can the **hyperparameters** be tuned to make it even better?\n","\n","**Hyperparameters** are like settings on a model you can adjust so some of the ways it uses to find patterns are altered and potentially improved. Adjusting hyperparameters is referred to as hyperparameter tuning."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"5CSwjfGrMQ3N","outputId":"c88164bd-667c-452e-9abb-03b4fcd37733"},"outputs":[{"data":{"text/plain":["{'bootstrap': True,\n"," 'ccp_alpha': 0.0,\n"," 'class_weight': None,\n"," 'criterion': 'gini',\n"," 'max_depth': None,\n"," 'max_features': 'sqrt',\n"," 'max_leaf_nodes': None,\n"," 'max_samples': None,\n"," 'min_impurity_decrease': 0.0,\n"," 'min_samples_leaf': 1,\n"," 'min_samples_split': 2,\n"," 'min_weight_fraction_leaf': 0.0,\n"," 'n_estimators': 100,\n"," 'n_jobs': None,\n"," 'oob_score': False,\n"," 'random_state': None,\n"," 'verbose': 0,\n"," 'warm_start': False}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# How to find a model's hyperparameters\n","clf = RandomForestClassifier()\n","clf.get_params() # returns a list of adjustable hyperparameters"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"2PJOsrBcMQ3N","outputId":"4dd4f5a3-ab4d-4590-b59c-b441d9c3fe77"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.868421052631579\n","0.8552631578947368\n"]}],"source":["# Example of adjusting hyperparameters by hand\n","\n","# Split data into X & y\n","X = heart_disease.drop(\"target\", axis=1) # use all columns except target\n","y = heart_disease[\"target\"] # we want to predict y using X\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y)\n","\n","# Instantiate two models with different settings\n","clf_1 = RandomForestClassifier(n_estimators=100)\n","clf_2 = RandomForestClassifier(n_estimators=200)\n","\n","# Fit both models on training data\n","clf_1.fit(X_train, y_train)\n","clf_2.fit(X_train, y_train)\n","\n","# Evaluate both models on test data and see which is best\n","print(clf_1.score(X_test, y_test))\n","print(clf_2.score(X_test, y_test))"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"B4Ds7rmIMQ3N","outputId":"29679dda-c036-4c4d-b5f2-821a531b4aee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 10 candidates, totalling 50 fits\n","[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   1.1s\n","[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   1.1s\n","[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   1.4s\n","[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   1.2s\n","[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.8s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.1s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.1s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alper\\AlperProjects\\UDEMY\\ML\\04-scikit-learn\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alper\\AlperProjects\\UDEMY\\ML\\04-scikit-learn\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alper\\AlperProjects\\UDEMY\\ML\\04-scikit-learn\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alper\\AlperProjects\\UDEMY\\ML\\04-scikit-learn\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alper\\AlperProjects\\UDEMY\\ML\\04-scikit-learn\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alper\\AlperProjects\\UDEMY\\ML\\04-scikit-learn\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alper\\AlperProjects\\UDEMY\\ML\\04-scikit-learn\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.4s\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alper\\AlperProjects\\UDEMY\\ML\\04-scikit-learn\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.4s\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alper\\AlperProjects\\UDEMY\\ML\\04-scikit-learn\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\alper\\AlperProjects\\UDEMY\\ML\\04-scikit-learn\\env\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END max_depth=30, max_features=auto, min_samples_leaf=2, min_samples_split=6, n_estimators=200; total time=   0.3s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.3s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.5s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.4s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.3s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=200; total time=   0.3s\n","[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=1200; total time=   2.2s\n","[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=1200; total time=   1.8s\n","[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=1200; total time=   1.6s\n","[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=1200; total time=   1.7s\n","[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=1200; total time=   1.5s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.0s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=500; total time=   0.5s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=500; total time=   0.5s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=500; total time=   0.5s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=500; total time=   0.5s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=500; total time=   0.6s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   0.1s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   0.1s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   0.0s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   0.1s\n","[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=4, n_estimators=100; total time=   0.0s\n","[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.2s\n","[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.2s\n","[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.2s\n","[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.2s\n","[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.2s\n","{'n_estimators': 500, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10}\n"]},{"data":{"text/plain":["0.7868852459016393"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Example of adjusting hyperparameters computationally (recommended)\n","\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","# Define a grid of hyperparameters\n","grid = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n","        \"max_depth\": [None, 5, 10, 20, 30],\n","        \"max_features\": [\"auto\", \"sqrt\"],\n","        \"min_samples_split\": [2, 4, 6],\n","        \"min_samples_leaf\": [1, 2, 4]}\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","# Set n_jobs to -1 to use all cores (NOTE: n_jobs=-1 is broken as of 8 Dec 2019, using n_jobs=1 works)\n","clf = RandomForestClassifier(n_jobs=1)\n","\n","# Setup RandomizedSearchCV\n","rs_clf = RandomizedSearchCV(estimator=clf,\n","                            param_distributions=grid,\n","                            n_iter=10, # try 10 models total\n","                            cv=5, # 5-fold cross-validation\n","                            verbose=2) # print out results\n","\n","# Fit the RandomizedSearchCV version of clf\n","rs_clf.fit(X_train, y_train);\n","\n","# Find the best hyperparameters\n","print(rs_clf.best_params_)\n","\n","# Scoring automatically uses the best hyperparameters\n","rs_clf.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"vR6dSyiuMQ3O"},"source":["## 6. Save and reload your trained model\n","You can save and load a model with `pickle`."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2eO_RfQaMQ3O"},"outputs":[],"source":["# Saving a model with pickle\n","import pickle\n","\n","# Save an existing model to file\n","pickle.dump(rs_clf, open(\"rs_random_forest_model_1.pkl\", \"wb\"))"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"tMjPNpoDMQ3O","outputId":"b48848f3-7a76-48a0-8a31-e116197372bc"},"outputs":[{"data":{"text/plain":["0.7868852459016393"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Load a saved pickle model\n","loaded_pickle_model = pickle.load(open(\"rs_random_forest_model_1.pkl\", \"rb\"))\n","\n","# Evaluate loaded model\n","loaded_pickle_model.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"d4q07cjdMQ3O"},"source":["You can do the same with `joblib`. `joblib` is usually more efficient with numerical data (what our models are)."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"QAk_iE1LMQ3O","outputId":"b36fbb59-db23-4aba-a95f-20fabc0aef04"},"outputs":[{"data":{"text/plain":["['gs_random_forest_model_1.joblib']"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Saving a model with joblib\n","from joblib import dump, load\n","\n","# Save a model to file\n","dump(rs_clf, filename=\"gs_random_forest_model_1.joblib\") "]},{"cell_type":"code","execution_count":20,"metadata":{"id":"GC2Zl1V4MQ3P"},"outputs":[],"source":["# Import a saved joblib model\n","loaded_joblib_model = load(filename=\"gs_random_forest_model_1.joblib\")"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"O6PuadIvMQ3P","outputId":"d6a20e95-d2ca-42fa-83e9-1484bae5dfa8"},"outputs":[{"data":{"text/plain":["0.7868852459016393"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Evaluate joblib predictions \n","loaded_joblib_model.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"ZkLbe-RTMQ3P"},"source":["## 7. Putting it all together (not pictured)\n","\n","We can put a number of different Scikit-Learn functions together using `Pipeline`.\n","\n","As an example, we'll use `car-sales-extended-missing-data.csv`. Which has missing data as well as non-numeric data. For a machine learning model to work, there can be no missing data or non-numeric values.\n","\n","The problem we're solving here is predicting a cars sales price given a number of parameters about the car (a regression problem)."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"W098AgGLMQ3P","outputId":"f4a8a375-c0a5-432c-a163-f14eec6d14da"},"outputs":[{"data":{"text/plain":["0.22188417408787875"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Getting data ready\n","import pandas as pd\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# Modelling\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","\n","# Setup random seed\n","import numpy as np\n","np.random.seed(42)\n","\n","# Import data and drop the rows with missing labels\n","data = pd.read_csv(\"car-sales-extended-missing-data.csv\")\n","data.dropna(subset=[\"Price\"], inplace=True)\n","\n","# Define different features and transformer pipelines\n","categorical_features = [\"Make\", \"Colour\"]\n","categorical_transformer = Pipeline(steps=[\n","    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n","    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n","\n","door_feature = [\"Doors\"]\n","door_transformer = Pipeline(steps=[\n","    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=4))])\n","\n","numeric_features = [\"Odometer (KM)\"]\n","numeric_transformer = Pipeline(steps=[\n","    (\"imputer\", SimpleImputer(strategy=\"mean\"))\n","])\n","\n","# Setup preprocessing steps (fill missing values, then convert to numbers)\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        (\"cat\", categorical_transformer, categorical_features),\n","        (\"door\", door_transformer, door_feature),\n","        (\"num\", numeric_transformer, numeric_features)])\n","\n","# Create a preprocessing and modelling pipeline\n","model = Pipeline(steps=[(\"preprocessor\", preprocessor),\n","                        (\"model\", RandomForestRegressor())])\n","\n","# Split data\n","X = data.drop(\"Price\", axis=1)\n","y = data[\"Price\"]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","# Fit and score the model\n","model.fit(X_train, y_train)\n","model.score(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/section-2-data-science-and-ml-tools/scikit-learn-what-were-covering.ipynb","timestamp":1669755666729}]},"kernelspec":{"display_name":"Python 3.10.8 (conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"5d49b017e43c354736e9fe425b9582ef6dbcfe4645c873e5380f35b053b4c6b0"}}},"nbformat":4,"nbformat_minor":0}
